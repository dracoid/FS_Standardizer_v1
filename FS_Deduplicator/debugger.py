# FS_Deduplicator/debugger.py
import pandas as pd
from FS_Deduplicator.utils import _compute_adsh_rank, _normalize_tag, _normalize_segment

def print_duplicate_groups_with_differences(df: pd.DataFrame) -> None:
    """
    ì¤‘ë³µ ì¡°ê±´ì— í•´ë‹¹í•˜ì§€ë§Œ ì œê±°ë˜ì§€ ì•Šì€ í•­ëª©ë“¤ì„ ì¶œë ¥í•˜ê³ ,
    ì–´ë–¤ ì»¬ëŸ¼ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— drop_duplicatesì—ì„œ ê±¸ëŸ¬ì§€ì§€ ì•ŠëŠ”ì§€ë¥¼ ë¹„êµ.
    """
    required_cols = {'ddate', 'tag', 'segments', 'qtrs'}
    if not required_cols.issubset(df.columns):
        print("âš ï¸ Required columns not found for duplicate debugging.")
        return

    df = _compute_adsh_rank(df)

    df['tag_tmp'] = df['tag'].astype(str).apply(_normalize_tag)
    df['segments_tmp'] = df['segments'].astype(str).apply(_normalize_segment)
    df['qtrs'] = pd.to_numeric(df['qtrs'], errors='coerce').fillna(0).astype(int)

    try:
        df['ddate'] = pd.to_datetime(df['ddate'], errors='coerce')
    except Exception as e:
        print(f"âŒ Failed to parse ddate: {e}")

    grouped = df.groupby(['ddate', 'tag_tmp', 'segments_tmp', 'qtrs'])

    for key, group in grouped:
        if len(group) > 1:
            diffs = group.drop(columns=['adsh_rank', 'tag_tmp', 'segments_tmp']).nunique()
            diff_cols = diffs[diffs > 1].index.tolist()
            if diff_cols:
                print(f"\nðŸŸ  Duplicate group with differences in {diff_cols}:")
                print(group[['adsh', 'value'] + diff_cols])